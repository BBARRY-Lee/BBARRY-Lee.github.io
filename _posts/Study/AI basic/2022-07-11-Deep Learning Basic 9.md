---
layout:   post
title:    "Deep Learning Basic 09 - CNN"
subtitle: "CNN"
category: Study
tags:     AI-Basic
image:
  path:   /assets/img/boostcourse.png
---
# Deep Learning Basic 09 - CNN

## 핵심목표
1. MLP : 만약 i 가 바뀌게 되면 사용하게 되는 가중치 행렬의 행도 바뀌기 때문에 가중치 행렬의 구조가 굉장히 커지게 됨 (실제 학습 시켜야하는 파라미터 숫자가 커짐)
2. Convolution 연산
  - 커널이라는 고정된 가중치 행렬을 사용하고, 이 고정된 커널을 입력벡터 상에서 움직여가며 선형모델과 합성함수가 적용되는 구조
  - 커널은 그대로 유지하고 x라는 입력벡터 상에서 커널 사이즈만큼 움직여가며 계산하는 것이 Conv 연산의 특징
  - Conv 연산의 핵심은 차원이 높아지고 i, j, k의 위치가 바뀌었을 때 커널(r)의 값은 바뀌지 않는다는 것과 커널이 위치에 따라서도 커널(r)의 값은 바뀌지 않는다는 것을 기억해야함 (기본 핵심)
3. 2D Convolution 연산 : 입력 행렬에 해당하는 데이터에서 커널을 x방향과 y방향으로 한 칸씩 움직여가며 적용
  ![](https://velog.velcdn.com/images/leejy1373/post/dd53b522-05d8-4f88-bc32-4c78b927aad2/image.png)
4. Convolution 연산의 역전파 : Conv 연산에 미분을 해도 똑같이 Conv이 나오는 것을 관찰 가능하며, Discrete일 때도 마찬가지로 성립
  ![](https://velog.velcdn.com/images/leejy1373/post/140c2ccc-d3b8-4b09-b901-e965242731a2/image.png)

## Intro
>- **`Convolution` 연산**과 **`다양한 차원`에서의 연산방법 공부**
- **Convolution 연산의 `역전파`**에 대해 공부
- Convolution 연산은 오늘날 굉장히 많은 모델에서 `이미지`나 `영상`을 처리하기 위해 사용
지금까지 배웠던 **`fully connected layer`**와 비교해서 CNN(Convolutional Neural Network)의 커널 연산이 가지는 **`장점`**과, Convolution 연산이 다양한 차원에서 **`어떻게 진행되는지`**를 이해
- Convolution 연산의 경우, 커널의 모든 입력데이터에 대해 공통으로 적용이 되기 때문에 역전파를 계산하는 경우에도 똑같이 Convolution 연산이 나오며, 이는 그림과 함께 잘 설명되어 있기 때문에 커널을 통해 **`gradient가 어떻게 전달`**이 되는지, **`역전파가 어떻게 이루어지는지 이해`**

---

<!--more-->

* this unordered seed list will be replaced by the toc
{:toc}

## 1. Convolutin 연산

![](https://velog.velcdn.com/images/leejy1373/post/be75c56c-c775-46bc-877e-0566780e1de5/image.png)


> **💡 복습**
- **`다층 신경망 (MLP)`** : 성분 $H_i$에 해당하는 가중치 행들이 각각의 i번째 위치마다 필요한 것이 다층신경망에서 사용되는 fully connected의 특징
  - 주어진 입력 벡터에 대해 가중치 행렬 $W$가있으면 $W_i$에 해당하는 가중치 행렬의 행과 입력 벡터의 내적을 통해, Hidden vector (잠재변수)의 $W_i$번째 행과 $x$(입력) 데이터를 통해 계산 가능

---

![](https://velog.velcdn.com/images/leejy1373/post/e5498d9a-1f19-496d-a2b1-2f0935e46331/image.png)


>- MLP는 $H_i$ 계산 시 $W$가 필요한데, 만약 $i$ 가 바뀌게 되면 사용하게 되는 가중치 행렬의 행도 바뀌기 때문에 가중치 행렬의 구조가 굉장히 커지게 됨 (실제 학습 시켜야하는 파라미터 숫자가 커짐)

---

![](https://velog.velcdn.com/images/leejy1373/post/2a82df35-0346-4e10-bdb8-f0aec87438a5/image.png)


>**💡 Convolution 연산**
- MLP 와 달리 **`커널`**이라는 **`고정된 가중치 행렬`**을 사용하고, 이 고정된 커널을 **입력벡터 상에서 움직여가며** 선형모델과 합성함수가 적용되는 구조
  - **`V = 커널`**이고 **`k = 커널 사이즈`**를 의미 (앞에서의 가중치 행렬과는 조금 다른 사이즈)
- 중요한 것은 입력 벡터 $x$를 모두 활용하는 것이 아닌, 커널 사이즈 $k$에 대응되는 사이즈만큼 입력벡터에서도 추출하게 됨
- 그래서 $i$번째 해당하는 $H_i$의 값을 계산할 때, `커널은 그대로 유지`하고 $x$라는 입력벡터 상에서 커널 사이즈만큼 움직여가며 계산하는 것이 Conv 연산의 특징

---

![](https://velog.velcdn.com/images/leejy1373/post/161ee723-a920-49a9-876b-e8ef4b1a6e1c/image.png)


>- 일반 선형모델과 조금 다른 형태인 부분은 만약 $i$가 바뀌게 되면 `활성함수`와 `커널`을 `제외`하고, Conv 연산이 $x$ 입력벡터 위에서 움직이며 적용됨
- 사실 이렇게 커널을 사용해 계산하는 Conv 연산도 선형변환의 한 종류인 것은 다름이 없음
다만, 가중치행렬이 $i$에 따라 바뀌는 것이 아닌, **`고정된 커널을 입력벡터 x 상에서 움직여가며 계산`**을 적용한다는 사실이 차이가 있으며, 이것이 Conv 연산의 기본적인 특징
- 이 커널을 공통적 커널을 사용해 연산에 활용하는 것이 특징이기 때문에, 어떤 $i$번째 위치에 따라 가중치 행이 따로 존재하지 않으며, $i$개수 상관없이 커널사이즈가 고정된 형태로 커널에 공통적으로 적용되기 때문에 `파라미터 사이즈를 많이 줄일 수 있음`

---

![](https://velog.velcdn.com/images/leejy1373/post/61eec06a-0f90-4d96-b8ce-e0bd163a9a06/image.png)


> - 정의역이 연속인 공간에서 `적분`을 사용해 정의
- 공간이 이상공간이면 적분이 불가해, `급수`로 표현할 수 있음
  - 단, 적분이나 급수이냐 차이일 뿐, 적용 방식은 똑같음
- 두 개의 함수 $f$, $g$가 있을 때 $x$라는 신호에 대한(입력) 값을 계산할 때,
전체 정의역에서  $f$, $g$를 각각 $z$를 움직여가며 두 함수를 곱해주고, $z$를 움직이면서 적분을 하거나 더해주는 형태로 Conv 연산을 실행하는데, 이 때 신호에 해당하는 것은 $g$, 커널에 해당하는 것은 $f$가 됨
- 신호와 커널의 역할을 바꿀 수 있음
- 위 수식에서 만약 $x-z$ 텀이 들어갔거나 $i-a$ 텀이 들어갔다면, 이를 `신호(입력) 텀 `이라고 할 수 있음
- `커널 텀`은 위 수식에서 $z$만 들어있거나 $a$만 들어있는 텀에 해당
- 그래서, Conv 연산의 수학적 의미는 신호와 커널을 이용해 `국소적인 증폭, 감소`를 통한 `정보를 추출` 또는 `필터링`하는 것이라고 할 수 있음

---

![](https://velog.velcdn.com/images/leejy1373/post/37c354a4-bdd2-440a-b705-1ec890e1bc0c/image.png)


> - 일반적으로 CNN에서 사용되는 연산은 엄밀히 빼기가 아닌 `덧셈`를 사용한 `cross-correlation`을 사용하기 때문에 정확한 용어는 cross-correlation 연산이라고 하는 게 맞음
- 그러나, 관용적으로 Conv 이라고 한 이유는 전체공간에서는 (+)(-)가 중요하지 않기 때문에 용어는 똑같이 성립 (다만 컴퓨터에서 Conv 연산을 사용할 때 (+)(-)에 따른 차이는 있음)

---

![](https://velog.velcdn.com/images/leejy1373/post/37fc8db0-b065-4d77-9b4c-82700f55bec2/image.png)

![](https://velog.velcdn.com/images/leejy1373/post/d685abba-8754-4ddf-93c1-ef724bcb76d2/image.png)



> **💡 Conv 연산을 그래픽컬하게 이해해보자.**
- **`파란색`** : 신호
- **`빨간색`** : 커널
- **`검은색`** : Conv 연산 결과
- **`빨간색`** : 커널은 정의역 내에서 계속 움직이지만 공통적으로 사용하기 때문에 변하지 않는 성질이 있어 `translation invariant`라고 하며, 파란색 신호에 대해 국소적(local)으로 계산한 결과로 Conv 연산을 실행하기 때문에 `locality`가 있다고 설명
- 빨간색 커널을 움직여가며 파란색 신호에 해당하는 연산에 적용 할 때, 노란색은 국소적(local)으로 적용되는 연산에 해당하는 것으로 그 결과는 검은색 (Conv 연산 결과)으로 나옴
- 파란색이 원래 신호에 해당했던 함수였으나 검은색으로 변환시켜,정보를 확산, 추출, 감소시키는 등의 역할을 Conv 연산이 수행

---

## 2. 영상처리에서 Convolution

![](https://velog.velcdn.com/images/leejy1373/post/d9a425f6-1449-4c34-b285-64cdc6b4a78a/image.png)


> **💡 Conv 연산은 영상처리에서 어떻게 이용 가능할까?**
- 다양한 종류의 Conv 연산이 사용되는 커널의 종류에 따라 영상이 어떻게 변하는지 관찰
- 커널 선택에 따라 영상에 적용했을 때 다양한 종류의 영상처리와 경계선 등 여러가지 Conv 연산을 통해 영상에서 쓸 수 있기 때문에 Conv 연산을 영상처리에서 많이 사용

---

## 3. 다양한 차원에서 Convolution

![](https://velog.velcdn.com/images/leejy1373/post/1c4b884d-0592-4a76-a1b9-5df21585af71/image.png)


>**💡 앞에서의 1차원 Conv 연산은 다차원에서 적용 가능**
  - 1D-Conv : 변수 1개에 대해 움직이는 것
  - 2D-Conv : 2개의 위치 $i$, $j$에 대해 두 개의 좌표를 `동시`에 움직이는 것
  - 3D-Conv : 3개의 좌표
- 데이터 성격에 따라 사용되는 커널의 종류가 다르기 때문에 2D, 3D의 사용여부는 `데이터 종류`에 따라 달라짐
- 음성이나 텍스트는 1D를 사용하는 경우가 많고, 흑백 영상은 2D, 컬러 영상은 3D와 같이 Conv 종류가 다름

---

![](https://velog.velcdn.com/images/leejy1373/post/18e32b81-3648-4473-8ebc-9959dce54433/image.png)

>- Conv 연산의 핵심은 차원이 높아지고 $i$, $j$, $k$의 위치가 바뀌었을 때 `커널(r)의 값`은 `바뀌지 않는다`는 것과 `커널이 위치`에 따라서도 `커널(r)의 값`은 `바뀌지 않는다`는 것을 기억해야함 (기본 핵심)

---

## 4. 2차원 Convolution 연산
![](https://velog.velcdn.com/images/leejy1373/post/b0fe1f2c-bb83-46dd-8f13-bd67480d239f/image.png)


>- 2D-Conv 연산은 영상에서 CNN을 많이 사용하는데, 2D-Conv 연산에서는 커널을 2D 행렬 모양에 해당하는 커널을 사용할 것
- 1D에서는 커널을 입력 벡터에서 **한 칸씩** 움직이면서 계산하는 것과 달리, 2D에서는 입력 행렬에 해당하는 데이터에서 커널을 **$x$방향과 $y$방향으로 한 칸씩** 움직여가며 적용

---

![](https://velog.velcdn.com/images/leejy1373/post/07b24ab6-3bac-4f66-a66c-97c34537ac83/image.png)


>- 2D-Conv 연산은 1차와 유사한데, $i, j$가 고정된 상황에서 Conv 위치에 해당하는 $p, q$를 움직여가며 계산하는 방식 $(i+p, j+q)$
- 그래서 2D `커널`이 `0, 1, 2, 3`으로 주어진 상황에서, 입력에서 커널의 크기만큼 적용해 계산하는 1차원 Conv 연산과 똑같음
- 왼쪽 수식으로 보면 $f(p, q)$ 와 같은 것이며 $p, q$가 각각 첫 번째와 두 번째 해당하는 위치
- (1,1) (1,2) (2,1) (2,2)에 해당하는 것이 `커널의 위치좌표`

---

** 💡 그림과 함께 이해하기**
- $p, q$의 변화에 따라 입력좌표에 $i, j$를 넣어주면, Conv 연산에 해당
- 입력에서는 커널사이즈에 맞춰 계산
- `0, 1, 2, 3` 크기에 맞춰 `입력 위치`에 해당하는 `0, 1, 3, 4`에 크기만큼 커널에 적용해 연산하면, 행렬연산이 아닌 `각각 위치에 따라 성분곱을 해서 더해주는 연산`을 해주게 됨
- 첫번째 위치에 해당하는 `0x0`을 더해주게 되고,`커널`의 첫번째 행 두번째 열에 해당하는 `1`과 `입력`의 첫번째 행 두번째 열에 해당하는 `1`을 곱해줘서 `1x1`을 더해주게 되고, `커널`의 두번째 행 첫번째 열 `입력`의 두번째 행과 첫번째 열에 해당는 값을 곱해줘서 `2x3`을 더해주고… (반복)
- 결과는 `19`가 되며 이는 `Conv 연산의 첫번째 계산 결과`

---

![](https://velog.velcdn.com/images/leejy1373/post/029cdc4e-911b-455d-8205-53a71adb603b/image.png)

> ** 💡 커널을 한 칸씩 옮겨서 계산한 것이 2D-Conv 연산** 
- 커널의 모양은 똑같이 유지한 상태에서 입력 행렬에서 한 칸 우측으로 이동해, 똑같이 연산
- 여기서 관찰해야할 것은 커널 값은 바뀌지 않고, 오로지 `입력 값`만 바뀌기 때문에 입력 값만 변경해서 계산
- 이처럼 커널을 가로, 세로로 한 칸씩 움직여가며 계산하는 것이 2D-Conv 연산
  - 0x1 + 1x2 + 2x4 + 3x5 = `25`

---

![](https://velog.velcdn.com/images/leejy1373/post/5ff35c56-e2f9-4888-ab1e-e3c8000d6083/image.png)


> - 입력크기와 커널 크기에 따라 계산되는 Conv 연산의 출력을 미리 예상할 수 있음
- 만약 입력의 크기가 $(H, W)$라고 하고 커널크기를 $(K_h, K_w)$ 출력크기$(O_h, O_w)$라고 하면, 계산과정을 쉽게 유추할 수 있음
  - 만약, 입력이 `28` by `28` 입력이고 `3` by `3` 커널로 2D-Conv 연산을 하면 `26` by `26`이 됨
    - 28-3`+1`이므로 26이 돼서 각각 가로랑 세로 크기가 `26` by `26` 이미지가 됨
- 이처럼 입력의 사이즈가 있고, 커널 사이즈가 있을 때 출력의 크기를 미리 예상 가능하고, Conv 연산에서 활용할 수 있으며 최종 결과물이 어떻게 나올지 예측 가능

---

![](https://velog.velcdn.com/images/leejy1373/post/72202542-7e7a-4b64-a9b6-cd818f558d84/image.png)


> - 실제 이미지 분석을 하는 경우, 2D 이미지이지만 채널이 여러 개 혹은 3D 입력 데이터를 갖는 경우가 많음
- RGB 데이터를 다루게 되면 2D 영상이지만 사실 3개의 채널이 존재하며, 투명도에 따라 채널이 4개일 수도 있음
- `3개 채널`이 존재하는 2D 이미지 영상의 경우, 마치 3차원 `입력`처럼 입력 데이터를 다루게 됨
- 이처럼 `채널이 여러 개`인 `2D 입력`의 경우, `3차원 입력`이 되고 `2D Conv`을 사용할 때 `채널 개수`만큼 `커널`을 만들어 적용하는 것으로 이해하면 됨
- 3D부터는 행렬이 아닌 **`tensor`**라고 부름

---

![](https://velog.velcdn.com/images/leejy1373/post/17f9d8e9-fc19-4964-9ce1-6d7e7927e4ac/image.png)


>- 채널이 여러 개인 2D-Conv의 경우, 2D 입력들을 분리한 것에서 각각 채널 개수만큼 커널을 만든 다음, 이 커널들을 2D 입력에 Conv 연산 적용 후 그 결과를 더해줘서 2D-Conv을 수행
>>(커널 `*` 2차원 입력) + (커널 `*` 2차원 입력) + (커널 `*` 2차원 입력)
- 2D 입력에서 `채널의 개수`만큼 `커널도 같은 개수`로 있어야만 2D-Conv 연산 수행가능
>> 채널이 여러 개인 경우, `커널의 채널 수 = 입력의 채널 수`

---

![](https://velog.velcdn.com/images/leejy1373/post/c22692af-1789-4bb1-95c2-173957b66adb/image.png)

> **💡  tensor를 직육면체 블럭으로 이해해보자.**
- 2D 입력에서 채널이 여러 개인 즉 3D 입력을 가지고 2D Conv 계산을 할 때, `커널`도 채널이 여러 개인 형태인 `tensor`로 이해할 수 있는데, 이때 커널들과 3D 입력 tensor를 Conv연산을 수행하면 다음과 같이 `출력과 채널이 1개인 출력으로 도출`이 됨
>> C - C + 1 = `1`
- 커널의 채널 개수와 입력 채널 개수를 같게 설정하고 Conv 연산 수행 후 더해지기 때문에, 출력의 tensor는 1이 되며 가로의 크기와 세로의 크기, $O$들은 앞의 계산이 그대로 반영해서 결과를 얻게 됨
- 이처럼 tensor를 직육면체 블럭으로 생각해서 2D-Conv을 이해하면, Conv 영상 절차를 이해하기 쉬움

---

![](https://velog.velcdn.com/images/leejy1373/post/11186c75-ea91-486d-8b64-06afd124c466/image.png)


> **💡 만약 출력이 여러 채널을 가지고 싶다면, 어떻게 Conv 연산을 수행하면 될까?**
- `커널 개수를 여러 개` 만들면 되며 출력의 `아웃풋 채널`을 $O_c$개 만큼 만들고 싶으면,
커널 개수를 $O_c$개씩 만든 후 Conv 연산을 각각 적용해서, $O_c$개의 채널을 가진 출력 tensor를 만드는 것이 가능
- 이런 연산들이 CNN에서 많이 사용되지만 이렇게 해야만 Conv 연산을 사용할 수 있는 건 아니며, 굉장히 많은 변형이 있기 때문에 기본적인 Conv 연산을 잘 이해해야 함

---

## 5. Convolution 연산의 역전파

![](https://velog.velcdn.com/images/leejy1373/post/acf38394-13d9-4e52-8745-9ffe50e4803d/image.png)


> - 역전파 Convn 연산도 선형변환인건 마찬가지며 MLP에서의 `역전파 계산방식과 동일`하게 계산하면 되는데, Conv 역전파를 면밀히 살펴보면 재미있는 특징이 있음
- Convn 연산은 커널의 모든 입력 데이터에 공통으로 적용되기 때문에 역전파 계산도 똑같이 Conv 연산이 나오게 됨

---


![](https://velog.velcdn.com/images/leejy1373/post/918505f7-3cf7-42c2-b403-c1adae05f6c6/image.png)
>- 수식으로 살펴보면, Conv에 해당하는 변수 $x$에 미분해서 미분 기호가 안에 들어가면 신호에 해당하는 부분에 미분이 적용되니 $g$의 도함수와 $f$가 Conv 연산을 수행하는 형태로 Conv 연산이 똑같이 나오게 됨
>> $[f * g'](x)$
- 즉 Conv 연산에 미분을 해도 `똑같이 Conv이 나오는 것`을 관찰 가능하며, Discrete일 때도 마찬가지로 성립

---

![](https://velog.velcdn.com/images/leejy1373/post/0279e0f7-af86-47e2-b5f1-fc8420546968/image.png)


**💡 과연 이것이 보통의 Convolution 연산과 똑같을까?**

> - 5개의 값을 가지는 입력벡터와 커널이 w1, w2, w3와 같이 3개로 이뤄진 1D-Conv가 있다면,
입력차원이 5개이고 커널차원이 3개가 되므로, 출력차원은 `5-3+1`=`3`이 되며, 출력벡터도 3개가 됨
>- 이 때 Conv 연산을 수행하면, 빨간 화살표는 w1을 연산하고, 파란색은 w2를 연산을 하고, 초록색은 w3을 연산하게 됨
- 이 때, 출력위치에 해당하는 O1, O2, O3이 Conv연산을 수행하며, 커널인 `w1, w2, w3`을 입력벡터에서 움직여가며 수행하는데, 화살표와 같이 w1, w2, w3 커널들의 값에 각각 적용하면서 O1, O2, O3을 계산

---

![](https://velog.velcdn.com/images/leejy1373/post/d04a8300-e10d-4cfb-962b-d9075aef9760/image.png)


>- 파란색 w2, 초록색 w3을 적용해서 연산하는데 각각 출력위치에 해당하는 O1, O2, O3의 Conv 연산을 수행해서 커널 개별적으로 입력벡터에서 움직여가며 수행

---

![](https://velog.velcdn.com/images/leejy1373/post/e07fecc4-ffee-44e6-afff-c91fce7f8df5/image.png)


![](https://velog.velcdn.com/images/leejy1373/post/79cfa2ad-4db2-4272-a4f6-0f1687208fd3/image.png)


>- Conv 연산을 수행하고 loss function에서 손실 값을 계산한 후, `역전파`가 오게 되면서 출력 백터에 해당하는 텀에서 벡터에 `δ1, δ2, δ3이라는 미분값`이 각각 `출력벡터의 위치에 전달`이 됨

---

![](https://velog.velcdn.com/images/leejy1373/post/70408e25-4a52-4a52-8c2a-9d6c965fe73e/image.png)

>**💡 이 때, 역전파 방식에서는 δ1, δ2, δ3인 gradient 벡터들이 어떤 식으로 X3과 커널에 전개될까?**
> - X3에서 각각 W1, W2, W3이 적용될 떄 `O1(δ1) → W3`, `O2(δ2) → W2`, `O3(δ3) → W1`이 적용된 것을 화살표를 통해 볼 수 있음
- 즉 X3 입력이 O1, O2, O3 적용 될 때, gradient 벡터에 대응되는 가중치들이 사용됨
- 다시 말해, O1 벡터는 X3와 W3이 곱해진 것이며, 이는 δ1에 W3가 곱해져서 X3로 전달된 것
  δ2에 W2가 곱해져서 X3 전달되고, O3에서는 X3가 W1이 곱해져서 왔기 때문에 δ3 gradient에 W1이 곱해져서 X3에 전달 되는 것
- 이와 같이 역전파 관계에서는 `입력벡터`에 곱해졌던 `커널`들을 통해 다시 `gradient`가 전달되는 것이며, 이는 `입력벡터`가 δ1, δ2, δ3과 w1, w2, w3이 `곱해져서 계산`되는 것으로 설명할 수 있음

---

![](https://velog.velcdn.com/images/leejy1373/post/7c5c8464-e56e-4af4-947a-3c0ff7b532b4/image.png)


>**💡 각각 커널들은 어떻게 gradient가 전달되게 되는 것일까?**
- O3는 `W1을 통해` X3으로 gradient를 전달했기 때문에,
W1을 통해 전달한 gradient인 δ3는 X3를 곱해서 `δ3X3` = `W1`의 gradient가 됨
- O2는 `W2을 통해` X3으로 gradient를 전달했기 때문에,
W2을 통해 전달한 gradient인 δ2는 X3를 곱해서 `δ2X3`를 W2의 gradient로 전달
- W3도 O1을 통해 전달된 δ1이 X3와 곱해져서 `δ1X3` W3의 gradient로 전달
- 즉, `커널`에 `δ`에 입력값 `X3`이 동일하게 `곱해져서` `전달`

---

![](https://velog.velcdn.com/images/leejy1373/post/d49504c0-305b-40d3-8e85-d78fce704184/image.png)


> - Conv 연산은 다른 모든 입력데이터에서도 커널이 공통으로 적용되기 때문에 역전파를 계산할 때도 gradient는 다른 입력에 적용된 gradient에 똑같이 적용이 되어,
  W1에 X1이 전달되고, O1도 W1로 전달되고 … (반복)
- 이를 이용해 각각 δ가 gradient를 통해 전달되고,x1, x2, x3가 각각 w1에 해당하는 gradient를 그림과 같이 전달
- 즉 각 커널에 들어오는 gradient를 더하면 결국, Conv 연산과 같음

---

<!--more-->
<br><br>

<div align="center">
소통은 제가 공부하고 공유하는 원동력이 됩니다.<br>
해당 글이 도움이 되셨다면 소중한 격려와 응원 부탁드립니다 ☺️
</div>  